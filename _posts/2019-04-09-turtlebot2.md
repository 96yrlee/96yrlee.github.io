---
layout: post
title: "Turtlebot2 Coding Challenges"
sub_title: "a"
categories:
  - Projects
tags:
- Coding
- Robotics
last_modified_at: 2019-04-09 
---


## Introduction 
This was for the UofT course MIE443: Mechatronics Systems: Integration and Design. Each group was assigned a Turtlebot2 to program control systems and complete three challenges:

1. [Mapping an enclosure](#1)
2. [Travel to and catalogue 5 objects before returning "home"](#2)
3. [Follow a user and react to the enviroment with emotions](#3)

I worked with Christine, Dave and Jack. Together, we all developed the strategies and control system architecture, wrote and tested the code on the turtlebot2, and wrote reports detailing our methodology and results.

## Basic Infomation

![img2](https://www.turtlebot.com/assets/images/turtlebot_2_lg.png){:width="225"} | A turtlebot is an open-sourced personal robot that runs on ROS. It comprises of a Kabuki base with motorized wheels and bumer sensor, a kinect sensor and some additional strcuture.

For each contest, we were restristed to using selected libraries. Specifically, in Contest1 we were not allowed to use the pre-package avoidance or exploration code, in Contest2 the path planning libraries and in Contest3 the user following libraries were off-limits.

### Mapping an Enclosure: Object Avoidance and Autonomous Mapping <a name="1"></a>
In this contest, the goal was to autonomously explore an unknown enclosure while using the built-in mapping capabilities. 

The objectives, in descending priorty, were to:
- Cover the entire area.
- Accurately map the area. Specifically, how similar the generated map is to the judge's created map.
- Reduce the time required, with a max limit of 8minutes.
- Avoid using the contact sensors: they were included to protect the robot, but were to be avoided.

The constraints were:
- that the map must use sensory information given by the Kinect.
- that the robot must be fully automonous. There must not be any human intervention
- a speed limit of 0.25m/s, and reduced to 0.1m/s when the robot is near obstacles
- that vel_pub was the only allowed publisher, thus eliminating pre-packaged exploration and object avoidance code.

I colloborated to tune the laser scan code and code the exploration strategy. 

# A collapsible section with markdown
<details>
  <summary>Click to expand!</summary>
  
  ## Heading
  1. A numbered
  2. list
     * With some
     * Sub bullets
</details>


<details>
  <summary>Click to see the Control Architecture Flowchart!</summary>
  
  #### Basic explanation on how the sensors, code and motors work together
  
  ![img2](/images/projects/turtlebot2/flowchart_controller_architecture.PNG "Contest 1 Flowchart")
  
  Diagram credit to Christine for drawing it 
  
</details>

<details>
  <summary>Click to see how the kinect sensor was used!</summary>
  
  #### Basic explanation on how we got distance data 
  
  ![img2](/images/projects/turtlebot2/discretation_process_laserCallback.PNG "How the Laser Works")
  
  Image credit to the course for providing it 
  
  The kinect is not an actual simple laser sensor. However, it can be used as such and the process for this was pre-packaged into a laser sensor array with over 600 elements. We discretized it into 10 by average the points for easier use and coding.
  
  With this, we now had information on how close an object was to the front of the Turtlebot2 at each angle - This formed the basis of our object avoidance algorithm and thus our exploration code.
    
</details>


### Find and Catalogue Objects: Path Planning and Image Identification <a name="2"></a>
In this contest, I was responsible for creating the path planning functions, adjusting the given coordinates of the boxes and sorting all the data so my group could use it. I also collaborated on testing the robot for all purposes.

#### Here's the code for my coordinate tuning and path planning functions
<details>
  <summary>Click to expand!</summary>
    
  ```
    test
    function whatIsLove() {
      console.log('Baby Don't hurt me. Don't hurt me');
      return 'No more';
    }
  ```
  
</details>

### Following a Person and Reacting: A Companionbot <a name="3"></a>
In this contest, I was responsible for creating a function that would recognizably emote as rage and ensuring the machine state triggered and transistioned out correctedly.  I also collaborated on testing the robot for all purposes.

<p>&nbsp;</p> 
<p>&nbsp;</p> 

## Notes
A fair amount of the writing was summarized and all the diagrams were pulled from our reports, credit to my teammates

Also, I consider this page to be incomplete, but sufficient. More details and edits to be added in time.









